import os
import sys
import wave
import contextlib
import subprocess
from faster_whisper import WhisperModel

def transcribe(audio_path: str, model_size: str = "small", language: str = "ar") -> str:
    # ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù…Ù„Ù Ù…ÙˆØ¬ÙˆØ¯
    if not os.path.exists(audio_path):
        print(f"âŒ Ø§Ù„Ù…Ù„Ù {audio_path} ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯.")
        return ""

    # ØªØ­Ù‚Ù‚ Ù…Ù† Ø·ÙˆÙ„ Ø§Ù„Ù…Ù„Ù Ø§Ù„ØµÙˆØªÙŠ
    with contextlib.closing(wave.open(audio_path, 'r')) as f:
        frames = f.getnframes()
        rate = f.getframerate()
        duration = frames / float(rate)
        if duration > 60:
            print("âš ï¸ Ø§Ù„ØªØ­Ø°ÙŠØ±: Ø§Ù„Ù…Ù„Ù Ø£Ø·ÙˆÙ„ Ù…Ù† Ø¯Ù‚ÙŠÙ‚Ø©. Ø³ÙŠØªÙ… Ø§Ù‚ØªØµØ§ØµÙ‡ Ù„Ø£ÙˆÙ„ 60 Ø«Ø§Ù†ÙŠØ©.")
            audio_path_cut = "/tmp/cut_audio.wav"
            subprocess.run(["ffmpeg", "-i", audio_path, "-t", "60", audio_path_cut, "-y"])
            audio_path = audio_path_cut

    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    model = WhisperModel(model_size, compute_type="int8")

    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ
    segments, _ = model.transcribe(audio_path, language=language)

    # Ø¬Ù…Ø¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    result = ""
    for segment in segments:
        result += segment.text + " "
    return result.strip()

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("âš ï¸ Ø§Ù„Ø§Ø³ØªØ¹Ù…Ø§Ù„: python transcriber.py path_to_audio.wav")
        sys.exit(1)
    
    audio_file = sys.argv[1]
    print("ğŸ” Ø¬Ø§Ø±ÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ...")
    text = transcribe(audio_file)
    print("\nğŸ“„ Ø§Ù„Ù†Øµ Ø§Ù„Ù†Ø§ØªØ¬:")
    print(text)
