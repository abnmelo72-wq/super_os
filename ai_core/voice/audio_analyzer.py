import os
import time
import sys

# ÙØ­Øµ Ø¥Ø°Ø§ Ø¯Ø§Ø®Ù„ Ø¨ÙŠØ¦Ø© venv
if sys.prefix == sys.base_prefix:
    print("âš ï¸ ÙŠØ¬Ø¨ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø³ÙƒØ±Ø¨Øª Ø¯Ø§Ø®Ù„ Ø¨ÙŠØ¦Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ© venv.")
    sys.exit(1)

# Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø­Ø³Ø¨ Ø³Ø±Ø¹Ø© Ø§Ù„Ø¬Ù‡Ø§Ø²
USE_WHISPER = True  # ØºÙŠÙ‘Ø± Ù„Ù€ False Ø¥Ø°Ø§ ÙƒØ§Ù† Ø¬Ù‡Ø§Ø²Ùƒ Ø¶Ø¹ÙŠÙ

# ØªØ­Ø¯ÙŠØ¯ Ù…Ù„Ù Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
OUTPUT_TEXT_FILE = "spoken_text.txt"
AUDIO_FILE = "command.wav"

# ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ffmpeg
print("ğŸ™ï¸ ØªØ­Ø¯Ø« Ø§Ù„Ø¢Ù† (5 Ø«ÙˆØ§Ù†ÙŠ)...")
os.system(f"ffmpeg -f alsa -i default -t 5 {AUDIO_FILE} -y > /dev/null 2>&1")
print("âœ… ØªÙ… Ø§Ù„ØªØ³Ø¬ÙŠÙ„... Ø¬Ø§Ø±Ù Ø§Ù„ØªØ­ÙˆÙŠÙ„")

# ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØª
text_result = ""

if USE_WHISPER:
    import whisper
    model = whisper.load_model("base")  # Ø§Ø®ØªØ± tiny Ø£Ùˆ base Ø­Ø³Ø¨ Ù‚ÙˆØ© Ø¬Ù‡Ø§Ø²Ùƒ
    result = model.transcribe(AUDIO_FILE)
    text_result = result["text"]
else:
    import vosk
    import soundfile as sf
    from vosk import Model, KaldiRecognizer
    import wave
    import json

    # ØªØ£ÙƒØ¯ Ø£Ù† Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ù…ÙˆØ¬ÙˆØ¯
    if not os.path.exists("vosk-model-small-ar-0.22"):
        print("âŒ Ù…ÙˆØ¯ÙŠÙ„ Vosk ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯! Ø­Ù…Ù‘Ù„Ù‡ Ù…Ù†: https://alphacephei.com/vosk/models")
        sys.exit(1)

    wf = wave.open(AUDIO_FILE, "rb")
    model = vosk.Model("vosk-model-small-ar-0.22")
    rec = KaldiRecognizer(model, wf.getframerate())

    while True:
        data = wf.readframes(4000)
        if len(data) == 0:
            break
        if rec.AcceptWaveform(data):
            res = json.loads(rec.Result())
            text_result += res.get("text", "") + " "

# Ø­ÙØ¸ Ø§Ù„Ù†Ø§ØªØ¬ ÙÙŠ Ù…Ù„Ù
with open(OUTPUT_TEXT_FILE, "w", encoding="utf-8") as f:
    f.write(text_result.strip())

print("ğŸ“„ Ø§Ù„Ø£Ù…Ø± Ø§Ù„ØµÙˆØªÙŠ ÙƒÙ†Øµ:")
print("ğŸ§ ", text_result.strip())
