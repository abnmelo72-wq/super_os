# context_engine.py
"""
ÙˆØ­Ø¯Ø© ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø°ÙƒÙŠ - Context Engine
Ù„Ù€ ObeyX ÙÙŠ Ù†Ø¸Ø§Ù… Super_OS
"""

import os
import json
import threading
import traceback
import time
import queue
from collections import defaultdict, deque
from datetime import datetime

# Ù…ÙƒØªØ¨Ø§Øª Ø¹Ù„Ù…ÙŠØ© ÙˆØªÙ‚Ù†ÙŠØ© Ù„Ø¯Ø¹Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ (Ù…Ù…ÙƒÙ† Ø¥Ø¶Ø§ÙØªÙ‡Ø§ Ù„Ø§Ø­Ù‚Ù‹Ø§)
try:
    import numpy as np
    import pandas as pd
except ImportError:
    # Ø¥Ø°Ø§ ØºÙŠØ± Ù…Ø«Ø¨ØªØ©ØŒ Ù†Ø±Ø³Ù„ ØªØ­Ø°ÙŠØ± ÙÙ‚Ø· ÙˆÙ„Ø§ Ù†ÙˆÙ‚Ù Ø§Ù„Ø¹Ù…Ù„
    print("[âš ï¸] Warning: numpy/pandas not found. Some advanced features disabled.")

# ----------------------------------------
# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ©
MAX_QUEUE_SIZE = 1000  # Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¹Ù„Ù‚Ø©
CACHE_EXPIRY_SECONDS = 600  # ÙˆÙ‚Øª ØµÙ„Ø§Ø­ÙŠØ© Ø§Ù„ÙƒØ§Ø´ 10 Ø¯Ù‚Ø§Ø¦Ù‚

# ----------------------------------------
# Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ© ÙˆØ°Ø§ÙƒØ±Ø© Ø§Ù„Ø³ÙŠØ§Ù‚
context_cache = {}
context_cache_timestamps = {}

# Ø³Ø¬Ù„ Ø§Ù„Ø£Ø­Ø¯Ø§Ø« ÙˆØ§Ù„Ø¹Ù…Ù„ÙŠØ§Øª (Ù…Ø¹ ØªØ®Ø²ÙŠÙ† Ù…Ø­Ø¯Ø¯)
event_log = deque(maxlen=5000)

# Ù‚Ø§Ø¦Ù…Ø© Ø§Ù†ØªØ¸Ø§Ø± Ù„Ù„Ø£ÙˆØ§Ù…Ø± Ø£Ùˆ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø±Ø§Ø¯ ØªØ­Ù„ÙŠÙ„Ù‡Ø§
input_queue = queue.Queue(maxsize=MAX_QUEUE_SIZE)

# ----------------------------------------
# Ø³Ø¬Ù„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ù…ØªÙƒØ±Ø± Ù„Ù…Ù†Ø¹ Ø§Ù„ØªÙˆÙ‚Ù Ø§Ù„ØªØ§Ù…
error_counter = defaultdict(int)

# ----------------------------------------
# Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù„Ù„ÙˆØ­Ø¯Ø©

def safe_execute(func):
    """
    Decorator Ù„Ø¶Ù…Ø§Ù† Ø¹Ø¯Ù… ØªÙˆÙ‚Ù ÙˆØ­Ø¯Ø© Ø§Ù„Ø³ÙŠØ§Ù‚ Ø¨Ø³Ø¨Ø¨ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ù…Ø¹ÙŠÙ†
    """
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            error_counter[func.__name__] += 1
            print(f"[â—] Error in {func.__name__}: {e}")
            traceback.print_exc()
            # Ø¹Ù†Ø¯ Ø­Ø¯ÙˆØ« Ø£ÙƒØ«Ø± Ù…Ù† 5 Ø£Ø®Ø·Ø§Ø¡ Ù…ØªØªØ§Ù„ÙŠØ© Ù†Ø±Ø³Ù„ Ø¥Ù†Ø°Ø§Ø± Ù„Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø¹Ù„Ù‰
            if error_counter[func.__name__] >= 5:
                notify_obeyx_critical(f"Repeated errors in {func.__name__}")
                error_counter[func.__name__] = 0
            return None
    return wrapper

def notify_obeyx_critical(message):
    """
    Ø¥Ø±Ø³Ø§Ù„ ØªÙ†Ø¨ÙŠÙ‡ Ù„Ù€ ObeyX Ø¨Ø®ØµÙˆØµ Ø£Ø®Ø·Ø§Ø¡ Ø­Ø±Ø¬Ø©
    """
    print(f"[ğŸš¨] Critical Alert to ObeyX: {message}")
    try:
        from obeyx_core.obeyx_interface import obeyx_alert
        obeyx_alert("context_engine", message)
    except ImportError:
        print("[âš ï¸] ObeyX interface not ready for alerts")

# ----------------------------------------
# Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„ÙƒØ§Ø´ (Ø­ÙØ¸ ÙˆØ§Ø³ØªØ±Ø¬Ø§Ø¹)

@safe_execute
def set_context_cache(key, value):
    context_cache[key] = value
    context_cache_timestamps[key] = time.time()

@safe_execute
def get_context_cache(key):
    if key in context_cache:
        age = time.time() - context_cache_timestamps.get(key, 0)
        if age < CACHE_EXPIRY_SECONDS:
            return context_cache[key]
        else:
            # Ø­Ø°Ù Ø§Ù„Ù‚ÙŠÙ…Ø© Ø¥Ø°Ø§ Ø§Ù†ØªÙ‡Øª ØµÙ„Ø§Ø­ÙŠØªÙ‡Ø§
            context_cache.pop(key, None)
            context_cache_timestamps.pop(key, None)
    return None

# ----------------------------------------
# Ø¥Ø¶Ø§ÙØ© Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù„ØªØ­Ù„ÙŠÙ„ (ØªØ¯ÙÙ‚ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª)

@safe_execute
def enqueue_input(data):
    """
    Ø¥Ø¶Ø§ÙØ© Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³ÙŠØ§Ù‚
    """
    if input_queue.full():
        print("[âš ï¸] Input queue full. Dropping oldest item to enqueue new data.")
        try:
            _ = input_queue.get_nowait()  # Ø¥Ø®Ø±Ø§Ø¬ Ø£Ù‚Ø¯Ù… Ø¹Ù†ØµØ±
        except queue.Empty:
            pass
    input_queue.put(data)

# ----------------------------------------
# Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø°ÙƒÙŠØ© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© (Ù…Ø­Ø§ÙƒØ§Ø© ØªØ­Ù„ÙŠÙ„ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª)

@safe_execute
def analyze_context(data):
    """
    ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© ÙˆØ¥Ø±Ø¬Ø§Ø¹ Ø§Ø³ØªÙ†ØªØ§Ø¬Ø§Øª Ø£Ùˆ Ø£ÙˆØ§Ù…Ø± Ø°ÙƒÙŠØ©.
    data: dict Ø£Ùˆ Ù†Øµ Ø£Ùˆ Ø£ÙŠ Ù†ÙˆØ¹ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    """
    # ØªØ¬Ø±Ø¨Ø© Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ÙƒØ§Ø´ Ø£ÙˆÙ„Ø§Ù‹
    cache_key = str(data)
    cached_result = get_context_cache(cache_key)
    if cached_result:
        return cached_result

    # ØªØ­Ù„ÙŠÙ„ Ø¨Ø³ÙŠØ· Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ (ÙŠÙ…ÙƒÙ† ØªØ¹Ù‚ÙŠØ¯Ù‡Ø§)
    result = {
        "timestamp": str(datetime.now()),
        "input_summary": str(data)[:100],  # Ù…Ù„Ø®Øµ Ø£ÙˆÙ„ 100 Ø­Ø±Ù
        "analysis": None,
        "recommendation": None,
        "confidence": 0.0
    }

    # Ù†ÙˆØ¹ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†ÙˆØ¹ Ø§Ù„Ø¯Ø®Ù„
    if isinstance(data, dict):
        # ØªØ­Ù„ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª JSON / Ø§Ù„Ù‡ÙŠØ§ÙƒÙ„
        result["analysis"] = f"Analyzed dict with {len(data)} keys."
        result["recommendation"] = "Process according to schema."
        result["confidence"] = 0.85
    elif isinstance(data, str):
        # ØªØ­Ù„ÙŠÙ„ Ù†ØµÙˆØµ (Ù…Ø«Ù„Ø§Ù‹ØŒ Ø£ÙˆØ§Ù…Ø± ØµÙˆØªÙŠØ© Ø£Ùˆ Ù†ØµÙˆØµ)
        if "error" in data.lower():
            result["analysis"] = "Detected error-related context."
            result["recommendation"] = "Trigger diagnostic module."
            result["confidence"] = 0.95
        else:
            result["analysis"] = "General text analysis performed."
            result["recommendation"] = "Forward to NLP or assistant."
            result["confidence"] = 0.75
    else:
        # Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙØ© Ø£Ùˆ Ù…Ù„ÙØ§Øª
        result["analysis"] = f"Received data of type {type(data).__name__}."
        result["recommendation"] = "Store for later deep analysis."
        result["confidence"] = 0.6

    # Ø­ÙØ¸ Ø§Ù„Ù†ØªÙŠØ¬Ø© ÙÙŠ Ø§Ù„ÙƒØ§Ø´
    set_context_cache(cache_key, result)
    # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø­Ø¯Ø«
    event_log.append({"time": datetime.now(), "data": data, "result": result})

    return result

# ----------------------------------------
# Ù…Ø±Ø§Ù‚Ø¨Ø© Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± ÙˆØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ø´ÙƒÙ„ Ù…Ø³ØªÙ…Ø± ÙÙŠ Ø®ÙŠØ· Ù…Ù†ÙØµÙ„

def process_input_queue():
    while True:
        try:
            data = input_queue.get()
            if data is None:
                break
            analyze_context(data)
        except Exception as e:
            print(f"[â—] Error processing queue: {e}")
            traceback.print_exc()
        time.sleep(0.01)  # ØªØ®ÙÙŠÙ Ø§Ù„Ø­Ù…Ù„ Ù‚Ù„ÙŠÙ„Ø§Ù‹

def start_context_engine():
    print("[ContextEngine] ğŸš€ Starting context analysis engine...")
    threading.Thread(target=process_input_queue, daemon=True).start()

# ----------------------------------------
# Ø¯Ø¹Ù… Ù…Ù„ÙØ§Øª ÙˆØ£Ù†ÙˆØ§Ø¹ Ù…ØªØ¹Ø¯Ø¯Ø© (Ù†Ù…Ø§Ø°Ø¬ Ù…Ø¨Ø³Ø·Ø©)

@safe_execute
def read_file_content(path):
    if not os.path.exists(path):
        return None
    with open(path, "r", encoding="utf-8") as f:
        return f.read()

@safe_execute
def save_file_content(path, content):
    with open(path, "w", encoding="utf-8") as f:
        f.write(content)
    return True

# ----------------------------------------
# Ø¯Ø¹Ù… Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ø°ÙˆÙØ© (Ù…Ø¨Ø³Ø·Ø©)

deleted_items = deque(maxlen=100)

@safe_execute
def backup_deleted_item(item):
    deleted_items.append(item)

@safe_execute
def recover_last_deleted():
    if deleted_items:
        return deleted_items.pop()
    return None

# ----------------------------------------
# Ø§Ø®ØªØ¨Ø§Ø± ÙˆØ­Ø¯Ø§Øª (Ù„ÙˆØ­Ø¯Ù‡Ø§)

if __name__ == "__main__":
    start_context_engine()
    print(analyze_context({"command": "ÙØªØ­ Ø§Ù„Ù…Ù„Ù", "file": "test.txt"}))
    enqueue_input("Ù‡Ø°Ø§ Ù†Øµ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø± ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³ÙŠØ§Ù‚ Ù…Ø¹ ObeyX")
    time.sleep(1)
    recovered = recover_last_deleted()
    print(f"Recovered deleted item: {recovered}")
